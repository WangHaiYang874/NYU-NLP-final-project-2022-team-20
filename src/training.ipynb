{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haiy/anaconda3/envs/tc113/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from Features import Features\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = '../data/mbti_full_pull_half_train.csv'\n",
    "df = pd.read_csv(train_csv_path, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbti_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>3279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>6614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>3757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>7209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            body\n",
       "mbti_type       \n",
       "ENFJ        3279\n",
       "ENFP       10000\n",
       "ENTJ        6614\n",
       "ENTP       10000\n",
       "ESFJ         346\n",
       "ESFP         695\n",
       "ESTJ        1043\n",
       "ESTP        3757\n",
       "INFJ       10000\n",
       "INFP       10000\n",
       "INTJ       10000\n",
       "INTP       10000\n",
       "ISFJ        1146\n",
       "ISFP        1453\n",
       "ISTJ        2380\n",
       "ISTP        7209"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is reducing the size of the training dataset\n",
    "new_indices = []\n",
    "for k,group in df.groupby([\"mbti_type\"]).groups.items():\n",
    "    if len(group) > 10000:\n",
    "        new_indices.extend(group[:10000])\n",
    "    else:\n",
    "        new_indices.extend(group)\n",
    "df = df.loc[new_indices]\n",
    "df.groupby(['mbti_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = '../models/features2021-12-13.model'\n",
    "\n",
    "try:\n",
    "    # the model can be loaded\n",
    "    with open(modelName,'rb') as f:\n",
    "        feature_extractor = pickle.load(f)\n",
    "except:\n",
    "    # training the model\n",
    "    feature_extractor = Features(df.body, '../data/stopwords.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the first layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enumerating all the cognitive functions (With repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I___\tE___\t_N__\t_S__\t__T_\t__F_\t___J\t___P\tIN__\tIS__\tEN__\tES__\tI_T_\tI_F_\tE_T_\tE_F_\tI__J\tI__P\tE__J\tE__P\t_NT_\t_NF_\t_ST_\t_SF_\t_N_J\t_N_P\t_S_J\t_S_P\t__TJ\t__TP\t__FJ\t__FP\tINT_\tINF_\tIST_\tISF_\tENT_\tENF_\tEST_\tESF_\tIN_J\tIN_P\tIS_J\tIS_P\tEN_J\tEN_P\tES_J\tES_P\tI_TJ\tI_TP\tI_FJ\tI_FP\tE_TJ\tE_TP\tE_FJ\tE_FP\t_NTJ\t_NTP\t_NFJ\t_NFP\t_STJ\t_STP\t_SFJ\t_SFP\tINTJ\tINTP\tINFJ\tINFP\tISTJ\tISTP\tISFJ\tISFP\tENTJ\tENTP\tENFJ\tENFP\tESTJ\tESTP\tESFJ\tESFP\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "types = ['IE','NS','TF','JP']\n",
    "\n",
    "deg1 = []\n",
    "\n",
    "for i in types:\n",
    "    for ii in i:\n",
    "        deg1.append(ii)\n",
    "\n",
    "deg2 = []\n",
    "for i,j, in combinations(types,2):\n",
    "    for ii in i:\n",
    "        for jj in j:\n",
    "            deg2.append(ii+jj)  \n",
    "\n",
    "deg3 = []\n",
    "for i,j,k in combinations(types,3):\n",
    "    for ii in i:\n",
    "        for jj in j:\n",
    "            for kk in k:\n",
    "                deg3.append(ii+jj+kk)\n",
    "\n",
    "deg4 = []\n",
    "for i,j,k,l in combinations(types,4):\n",
    "    for ii in i:\n",
    "        for jj in j:\n",
    "            for kk in k:\n",
    "                for ll in l:\n",
    "                    deg4.append(ii+jj+kk+ll)\n",
    "\n",
    "cog_funs = deg1 + deg2 + deg3 + deg4\n",
    "\n",
    "def normalize(s):\n",
    "    ret = ''\n",
    "    for type in types:\n",
    "        if type[0] in s:\n",
    "            ret += type[0]\n",
    "        elif type[1] in s:\n",
    "            ret += type[1]\n",
    "        else:\n",
    "            ret += '_'\n",
    "    return ret\n",
    "\n",
    "cog_funs = list(map(normalize,cog_funs))\n",
    "print('\\t'.join(cog_funs))\n",
    "print(len(cog_funs))\n",
    "cog_funs = {i:None for i in cog_funs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are repeating elements in the above listed cog_funs, such as 'I___' and 'E___' are really the same thing, and I choose not to handle this repeatition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what's left to do is to have feature extractions and then train binary classifier for each cognitive functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the feature extraction is trained and stored in a ../models/features______.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_match(y,y_):\n",
    "    for i,j in enumerate(y_):\n",
    "        if j == '_':\n",
    "            pass\n",
    "        elif j == y[i]:\n",
    "            pass\n",
    "        else:\n",
    "            return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_match('INTJ','_NT_'), check_match('INTJ','E___')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the first layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNameSuffix = '2021-12-13'\n",
    "\n",
    "train_X = feature_extractor.get_features(df.body)\n",
    "train_y = df.mbti_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I___ training completed\n",
      "E___ training completed\n",
      "_N__ training completed\n",
      "_S__ training completed\n",
      "__T_ training completed\n",
      "__F_ training completed\n",
      "___J training completed\n",
      "___P training completed\n",
      "IN__ training completed\n",
      "IS__ training completed\n",
      "EN__ training completed\n",
      "ES__ training completed\n",
      "I_T_ training completed\n",
      "I_F_ training completed\n",
      "E_T_ training completed\n",
      "E_F_ training completed\n",
      "I__J training completed\n",
      "I__P training completed\n",
      "E__J training completed\n",
      "E__P training completed\n",
      "_NT_ training completed\n",
      "_NF_ training completed\n",
      "_ST_ training completed\n",
      "_SF_ training completed\n",
      "_N_J training completed\n",
      "_N_P training completed\n",
      "_S_J training completed\n",
      "_S_P training completed\n",
      "__TJ training completed\n",
      "__TP training completed\n",
      "__FJ training completed\n",
      "__FP training completed\n",
      "INT_ training completed\n",
      "INF_ training completed\n",
      "IST_ training completed\n",
      "ISF_ training completed\n",
      "ENT_ training completed\n",
      "ENF_ training completed\n",
      "EST_ training completed\n",
      "ESF_ training completed\n",
      "IN_J training completed\n",
      "IN_P training completed\n",
      "IS_J training completed\n",
      "IS_P training completed\n",
      "EN_J training completed\n",
      "EN_P training completed\n",
      "ES_J training completed\n",
      "ES_P training completed\n",
      "I_TJ training completed\n",
      "I_TP training completed\n",
      "I_FJ training completed\n",
      "I_FP training completed\n",
      "E_TJ training completed\n",
      "E_TP training completed\n",
      "E_FJ training completed\n",
      "E_FP training completed\n",
      "_NTJ training completed\n",
      "_NTP training completed\n",
      "_NFJ training completed\n",
      "_NFP training completed\n",
      "_STJ training completed\n",
      "_STP training completed\n",
      "_SFJ training completed\n",
      "_SFP training completed\n",
      "INTJ training completed\n",
      "INTP training completed\n",
      "INFJ training completed\n",
      "INFP training completed\n",
      "ISTJ training completed\n",
      "ISTP training completed\n",
      "ISFJ training completed\n",
      "ISFP training completed\n",
      "ENTJ training completed\n",
      "ENTP training completed\n",
      "ENFJ training completed\n",
      "ENFP training completed\n",
      "ESTJ training completed\n",
      "ESTP training completed\n",
      "ESFJ training completed\n",
      "ESFP training completed\n"
     ]
    }
   ],
   "source": [
    "for model in cog_funs:\n",
    "    \n",
    "    train_yy = [check_match(i,model) for i in train_y]\n",
    "    \n",
    "    classifier = RandomForestClassifier(n_estimators=10)\n",
    "    \n",
    "    classifier.fit(train_X, train_yy)\n",
    "    \n",
    "    with open('../models/first_layer/'  + model + modelNameSuffix + '.model','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "        cog_funs[model] = classifier\n",
    "    print(model +' training completed', end='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/mbti_full_pull_half_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = feature_extractor.get_features(test_df.body)\n",
    "test_y = test_df.mbti_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_(cog):\n",
    "    with open(\"../models/first_layer/\"+cog+modelNameSuffix + '.model','rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    predict_y = model.predict(test_X)\n",
    "    counter = 0\n",
    "    test_yy = [check_match(cog, c) for c in  test_y]\n",
    "    for x,y in zip(test_yy, predict_y):\n",
    "        if x==y:\n",
    "            counter+=1;\n",
    "    print(counter/len(predict_y))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I___\n",
      "0.36212808317663975\n",
      "E___\n",
      "0.8268259410832135\n",
      "_N__\n",
      "0.07757253991077682\n",
      "_S__\n",
      "0.9710577738450762\n",
      "__T_\n",
      "0.43914758691885114\n",
      "__F_\n",
      "0.7595398739077536\n",
      "___J\n",
      "0.8019393134977694\n",
      "___P\n",
      "0.3964163256276961\n",
      "IN__\n",
      "0.6611363049810124\n",
      "IS__\n",
      "0.9894923127972569\n",
      "EN__\n",
      "0.8945175681156214\n",
      "ES__\n",
      "0.9965343066769901\n",
      "I_T_\n",
      "0.8668657596873502\n",
      "I_F_\n",
      "0.9484201600117981\n",
      "E_T_\n",
      "0.9588172399808281\n",
      "E_F_\n",
      "0.9865059174870037\n",
      "I__J\n",
      "0.9337831360837665\n",
      "I__P\n",
      "0.8903144932345242\n",
      "E__J\n",
      "0.9873170372008996\n",
      "E__P\n",
      "0.9404195701065516\n",
      "_NT_\n",
      "0.747151863731888\n",
      "_NF_\n",
      "0.8229915569811599\n",
      "_ST_\n",
      "0.9820078899826715\n",
      "_SF_\n",
      "0.9994469638314346\n",
      "_N_J\n",
      "0.8686354754267596\n",
      "_N_P\n",
      "0.6985215499760351\n",
      "_S_J\n",
      "0.9984883678059212\n",
      "_S_P\n",
      "0.9835563912546548\n",
      "__TJ\n",
      "0.9593702761493935\n",
      "__TP\n",
      "0.8596762894959997\n",
      "__FJ\n",
      "0.9784315894259484\n",
      "__FP\n",
      "0.9525126276591822\n",
      "INT_\n",
      "0.9431478818714744\n",
      "INF_\n",
      "0.9627991003944991\n",
      "IST_\n",
      "0.9927736607307451\n",
      "ISF_\n",
      "0.9995207019872433\n",
      "ENT_\n",
      "0.9744865980901818\n",
      "ENF_\n",
      "0.9886443240054567\n",
      "EST_\n",
      "0.9966817829886074\n",
      "ESF_\n",
      "0.9999631309220957\n",
      "IN_J\n",
      "0.9530656638277477\n",
      "IN_P\n",
      "0.9571212623972274\n",
      "IS_J\n",
      "0.9994100947535303\n",
      "IS_P\n",
      "0.9927367916528408\n",
      "EN_J\n",
      "0.9881650259927\n",
      "EN_P\n",
      "0.9651587213803783\n",
      "ES_J\n",
      "0.999483832909339\n",
      "ES_P\n",
      "0.9971979500792685\n",
      "I_TJ\n",
      "0.9825609261512369\n",
      "I_TP\n",
      "0.9671127825093094\n",
      "I_FJ\n",
      "0.9858054050068208\n",
      "I_FP\n",
      "0.9922206245621797\n",
      "E_TJ\n",
      "0.9932160896655975\n",
      "E_TP\n",
      "0.9840356892674115\n",
      "E_FJ\n",
      "0.9980828079489732\n",
      "E_FP\n",
      "0.9916675883936142\n",
      "_NTJ\n",
      "0.9712789883125023\n",
      "_NTP\n",
      "0.9539873907753567\n",
      "_NFJ\n",
      "0.98613722670796\n",
      "_NFP\n",
      "0.9638683036537257\n",
      "_STJ\n",
      "0.9990045348965823\n",
      "_STP\n",
      "0.9854735833056816\n",
      "_SFJ\n",
      "0.999594440143052\n",
      "_SFP\n",
      "0.9997050473767651\n",
      "INTJ\n",
      "0.7780112819378388\n",
      "INTP\n",
      "0.7755410537182466\n",
      "INFJ\n",
      "0.8497585075397265\n",
      "INFP\n",
      "0.8934852339342993\n",
      "ISTJ\n",
      "0.9895291818751613\n",
      "ISTP\n",
      "0.9706890830660325\n",
      "ISFJ\n",
      "0.9941378166132065\n",
      "ISFP\n",
      "0.9941378166132065\n",
      "ENTJ\n",
      "0.9745603362459905\n",
      "ENTP\n",
      "0.8945913062714301\n",
      "ENFJ\n",
      "0.9857316668510121\n",
      "ENFP\n",
      "0.9404933082623603\n",
      "ESTJ\n",
      "0.9962762231316594\n",
      "ESTP\n",
      "0.9839619511116027\n",
      "ESFJ\n",
      "0.9986358441175386\n",
      "ESFP\n",
      "0.9969767356118423\n"
     ]
    }
   ],
   "source": [
    "for cog in cog_funs:\n",
    "    print(cog,end=':\\t')\n",
    "    test(cog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_predict(train_X):\n",
    "    return np.array([cog_funs[model].predict_proba(train_X)[:,0] \\\n",
    "        for model in sorted(list(cog_funs))]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The second layer model: random forest\n",
    "\n",
    "The inputs of the second layer model should be\n",
    "- ✔ cognitive functions, there are roughly 80 of them. With a bigger weight\n",
    "- ❌ the features. \n",
    "\n",
    "We have imagined to use a second layer as a NN, which takes both the cognitive functions and the features. However, we realized that NN is too costly and really not necessary, as we have a first layer with incredible accuracy. So we will use a simple _random forest_ for the second layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_types = ['ENFJ','ENFP','ENTJ','ENTP','ESFJ','ESFP','ESTJ','ESTP',\n",
    "    'INFJ','INFP','INTJ','INTP','ISFJ','ISFP','ISTJ','ISTP']\n",
    "type2int = {t:i for i,t in enumerate(mbti_types)}\n",
    "int2type = {i:t for i,t in enumerate(mbti_types)}\n",
    "\n",
    "train_y2 = train_y.apply(lambda x:type2int[x]).values\n",
    "test_y2  = test_y.apply(lambda x:type2int[x]).values\n",
    "\n",
    "train_X2 = cf_predict(train_X)\n",
    "test_X2  = cf_predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_layer_classifier = RandomForestClassifier(n_estimators=100)\n",
    "second_layer_classifier.fit(train_X2,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y2, second_layer_classifier.predict(test_X2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
