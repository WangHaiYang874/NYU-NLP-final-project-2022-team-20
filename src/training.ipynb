{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from Features import Features\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = '../data/mbti_full_pull_half_train.csv'\n",
    "# train_csv_path = '/mnt/c/Users/haiya/Downloads/finalp/mbti_full_pull_half_train.csv'\n",
    "\n",
    "df = pd.read_csv(train_csv_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mbti_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>3279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>6614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>3757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>7209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            body\n",
       "mbti_type       \n",
       "ENFJ        3279\n",
       "ENFP       10000\n",
       "ENTJ        6614\n",
       "ENTP       10000\n",
       "ESFJ         346\n",
       "ESFP         695\n",
       "ESTJ        1043\n",
       "ESTP        3757\n",
       "INFJ       10000\n",
       "INFP       10000\n",
       "INTJ       10000\n",
       "INTP       10000\n",
       "ISFJ        1146\n",
       "ISFP        1453\n",
       "ISTJ        2380\n",
       "ISTP        7209"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is reducing the size of the training dataset\n",
    "new_indices = []\n",
    "for k,group in df.groupby([\"mbti_type\"]).groups.items():\n",
    "    if len(group) > 10000:\n",
    "        new_indices.extend(group[:10000])\n",
    "    else:\n",
    "        new_indices.extend(group)\n",
    "df = df.loc[new_indices]\n",
    "df.groupby(['mbti_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning the corpora\n",
      "building tfidf model\n",
      "building lda topic model\n",
      "model built, saving it\n",
      "model saved at:\n"
     ]
    }
   ],
   "source": [
    "modelName = '../models/features2021-12-13.model'\n",
    "\n",
    "try:\n",
    "    # the model can be loaded\n",
    "    with open(modelName,'rb') as f:\n",
    "        feature_extractor = pickle.load(f)\n",
    "except:\n",
    "    # training the model\n",
    "    feature_extractor = Features(df.body, '../data/stopwords.txt')\n",
    "    feature_extractor.build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the first layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enumerating all the cognitive functions (With repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I___\tE___\t_N__\t_S__\t__T_\t__F_\t___J\t___P\tIN__\tIS__\tEN__\tES__\tI_T_\tI_F_\tE_T_\tE_F_\tI__J\tI__P\tE__J\tE__P\t_NT_\t_NF_\t_ST_\t_SF_\t_N_J\t_N_P\t_S_J\t_S_P\t__TJ\t__TP\t__FJ\t__FP\tINT_\tINF_\tIST_\tISF_\tENT_\tENF_\tEST_\tESF_\tIN_J\tIN_P\tIS_J\tIS_P\tEN_J\tEN_P\tES_J\tES_P\tI_TJ\tI_TP\tI_FJ\tI_FP\tE_TJ\tE_TP\tE_FJ\tE_FP\t_NTJ\t_NTP\t_NFJ\t_NFP\t_STJ\t_STP\t_SFJ\t_SFP\tINTJ\tINTP\tINFJ\tINFP\tISTJ\tISTP\tISFJ\tISFP\tENTJ\tENTP\tENFJ\tENFP\tESTJ\tESTP\tESFJ\tESFP\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "types = ['IE','NS','TF','JP']\n",
    "\n",
    "deg1 = []\n",
    "\n",
    "for i in types:\n",
    "    for ii in i:\n",
    "        deg1.append(ii)\n",
    "\n",
    "deg2 = []\n",
    "for i,j, in combinations(types,2):\n",
    "    for ii in i:\n",
    "        for jj in j:\n",
    "            deg2.append(ii+jj)  \n",
    "\n",
    "deg3 = []\n",
    "for i,j,k in combinations(types,3):\n",
    "    for ii in i:\n",
    "        for jj in j:\n",
    "            for kk in k:\n",
    "                deg3.append(ii+jj+kk)\n",
    "\n",
    "deg4 = []\n",
    "for i,j,k,l in combinations(types,4):\n",
    "    for ii in i:\n",
    "        for jj in j:\n",
    "            for kk in k:\n",
    "                for ll in l:\n",
    "                    deg4.append(ii+jj+kk+ll)\n",
    "\n",
    "cog_funs = deg1 + deg2 + deg3 + deg4\n",
    "\n",
    "def normalize(s):\n",
    "    ret = ''\n",
    "    for type in types:\n",
    "        if type[0] in s:\n",
    "            ret += type[0]\n",
    "        elif type[1] in s:\n",
    "            ret += type[1]\n",
    "        else:\n",
    "            ret += '_'\n",
    "    return ret\n",
    "\n",
    "cog_funs = list(map(normalize,cog_funs))\n",
    "print('\\t'.join(cog_funs))\n",
    "print(len(cog_funs))\n",
    "cog_funs = {i:None for i in cog_funs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are repeating elements in the above listed cog_funs, such as 'I___' and 'E___' are really the same thing, and I choose not to handle this repeatition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what's left to do is to have feature extractions and then train binary classifier for each cognitive functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the feature extraction is trained and stored in a ../models/features______.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_match(y,y_):\n",
    "    for i,j in enumerate(y_):\n",
    "        if j == '_':\n",
    "            pass\n",
    "        elif j == y[i]:\n",
    "            pass\n",
    "        else:\n",
    "            return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_match('INTJ','_NT_'), check_match('INTJ','E___')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the first layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNameSuffix = '2021-12-13'\n",
    "\n",
    "train_X = feature_extractor.get_features(df.body)\n",
    "train_y = df.mbti_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I___ training completed\tE___ training completed\t_N__ training completed\t_S__ training completed\t__T_ training completed\t__F_ training completed\t___J training completed\t___P training completed\tIN__ training completed\tIS__ training completed\tEN__ training completed\tES__ training completed\tI_T_ training completed\tI_F_ training completed\tE_T_ training completed\tE_F_ training completed\tI__J training completed\tI__P training completed\tE__J training completed\tE__P training completed\t_NT_ training completed\t_NF_ training completed\t_ST_ training completed\t_SF_ training completed\t_N_J training completed\t_N_P training completed\t_S_J training completed\t_S_P training completed\t__TJ training completed\t__TP training completed\t__FJ training completed\t__FP training completed\tINT_ training completed\tINF_ training completed\tIST_ training completed\tISF_ training completed\tENT_ training completed\tENF_ training completed\tEST_ training completed\tESF_ training completed\tIN_J training completed\tIN_P training completed\tIS_J training completed\tIS_P training completed\tEN_J training completed\tEN_P training completed\tES_J training completed\tES_P training completed\tI_TJ training completed\tI_TP training completed\tI_FJ training completed\tI_FP training completed\tE_TJ training completed\tE_TP training completed\tE_FJ training completed\tE_FP training completed\t_NTJ training completed\t_NTP training completed\t_NFJ training completed\t_NFP training completed\t_STJ training completed\t_STP training completed\t_SFJ training completed\t_SFP training completed\tINTJ training completed\tINTP training completed\tINFJ training completed\tINFP training completed\tISTJ training completed\tISTP training completed\tISFJ training completed\tISFP training completed\tENTJ training completed\tENTP training completed\tENFJ training completed\tENFP training completed\tESTJ training completed\tESTP training completed\tESFJ training completed\tESFP training completed\t"
     ]
    }
   ],
   "source": [
    "for model in cog_funs.keys():\n",
    "    \n",
    "    train_yy = [check_match(i,model) for i in train_y]\n",
    "    \n",
    "    classifier = RandomForestClassifier(n_estimators=10)\n",
    "    \n",
    "    classifier.fit(train_X, train_yy)\n",
    "    \n",
    "    with open('../models/first_layer/'  + model + modelNameSuffix + '.model','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "        cog_funs[model] = classifier\n",
    "    print(model +' training completed', end='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = '../data/mbti_full_pull_half_test.csv'\n",
    "# test_csv = '/mnt/c/Users/haiya/Downloads/finalp/mbti_full_pull_half_test.csv'\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(test_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = feature_extractor.get_features(test_df.body)\n",
    "test_y = test_df.mbti_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(cog):\n",
    "    with open(\"../models/first_layer/\"+cog+modelNameSuffix + '.model','rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    predict_y = model.predict(test_X)\n",
    "    counter = 0\n",
    "    test_yy = [check_match(cog, c) for c in  test_y]\n",
    "    for x,y in zip(test_yy, predict_y):\n",
    "        if x==y:\n",
    "            counter+=1;\n",
    "    print(counter/len(predict_y))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I___:\t0.3308631051137411\n",
      "E___:\t0.826162297680935\n",
      "_N__:\t0.05264904324742838\n",
      "_S__:\t0.9733436566751466\n",
      "__T_:\t0.42148729860266193\n",
      "__F_:\t0.7555948825719869\n",
      "___J:\t0.8028610404453784\n",
      "___P:\t0.3634922390591011\n",
      "IN__:\t0.6515503447258784\n",
      "IS__:\t0.9902296943553441\n",
      "EN__:\t0.8963241529329351\n",
      "ES__:\t0.9965343066769901\n",
      "I_T_:\t0.8642480551561406\n",
      "I_F_:\t0.952291413191756\n",
      "E_T_:\t0.9594440143052022\n",
      "E_F_:\t0.986653393798621\n",
      "I__J:\t0.9399402720937949\n",
      "I__P:\t0.8863326328208532\n",
      "E__J:\t0.9898241344983962\n",
      "E__P:\t0.9368432695498286\n",
      "_NT_:\t0.7494377465619585\n",
      "_NF_:\t0.8241713674740995\n",
      "_ST_:\t0.9856579286952033\n",
      "_SF_:\t0.9995207019872433\n",
      "_N_J:\t0.8688566898941857\n",
      "_N_P:\t0.6973417394830955\n",
      "_S_J:\t0.9984146296501124\n",
      "_S_P:\t0.983998820189507\n",
      "__TJ:\t0.9622829333038381\n",
      "__TP:\t0.8559893817055635\n",
      "__FJ:\t0.9796482689967924\n",
      "__FP:\t0.9561995354496184\n",
      "INT_:\t0.9550934631124876\n",
      "INF_:\t0.9685138074696752\n",
      "IST_:\t0.9931423515097887\n",
      "ISF_:\t0.9992626184419128\n",
      "ENT_:\t0.9775098624783394\n",
      "ENF_:\t0.9878332042915606\n",
      "EST_:\t0.9966817829886074\n",
      "ESF_:\t0.999889392766287\n",
      "IN_J:\t0.9525494967370866\n",
      "IN_P:\t0.9607713011097593\n",
      "IS_J:\t0.9994469638314346\n",
      "IS_P:\t0.9919994100947536\n",
      "EN_J:\t0.9888655384728828\n",
      "EN_P:\t0.9663016627954135\n",
      "ES_J:\t0.999483832909339\n",
      "ES_P:\t0.996939866533938\n",
      "I_TJ:\t0.9838144747999853\n",
      "I_TP:\t0.9669653061976919\n",
      "I_FJ:\t0.9896397891088744\n",
      "I_FP:\t0.9915201120819969\n",
      "E_TJ:\t0.9923312317958928\n",
      "E_TP:\t0.9855841905393946\n",
      "E_FJ:\t0.9984883678059212\n",
      "E_FP:\t0.9903403015890573\n",
      "_NTJ:\t0.9661910555617004\n",
      "_NTP:\t0.958706632747115\n",
      "_NFJ:\t0.9859897503963426\n",
      "_NFP:\t0.9701360468974671\n",
      "_STJ:\t0.9985621059617299\n",
      "_STP:\t0.986432179331195\n",
      "_SFJ:\t0.9996681782988608\n",
      "_SFP:\t0.999594440143052\n",
      "INTJ:\t0.7821037495852229\n",
      "INTP:\t0.7752092320171072\n",
      "INFJ:\t0.8513807469675183\n",
      "INFP:\t0.894886258894665\n",
      "ISTJ:\t0.9893448364856395\n",
      "ISTP:\t0.9713158573904067\n",
      "ISFJ:\t0.9937691258341629\n",
      "ISFP:\t0.9942852929248239\n",
      "ENTJ:\t0.9750027651808428\n",
      "ENTP:\t0.895291818751613\n",
      "ENFJ:\t0.9858422740847251\n",
      "ENFP:\t0.9416731187552999\n",
      "ESTJ:\t0.9963499612874682\n",
      "ESTP:\t0.9845887254359769\n",
      "ESFJ:\t0.9986358441175386\n",
      "ESFP:\t0.9970136046897468\n"
     ]
    }
   ],
   "source": [
    "for cog in cog_funs:\n",
    "    print(cog,end=':\\t')\n",
    "    test(cog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_predict(train_X):\n",
    "    return np.array([cog_funs[model].predict_proba(train_X)[:,0] \\\n",
    "        for model in sorted(list(cog_funs))]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The second layer model: random forest\n",
    "\n",
    "The inputs of the second layer model should be\n",
    "- ✔ cognitive functions, there are roughly 80 of them. With a bigger weight\n",
    "- ❌ the features. \n",
    "\n",
    "We have imagined to use a second layer as a NN, which takes both the cognitive functions and the features. However, we realized that NN is too costly and really not necessary, as we have a first layer with incredible accuracy. So we will use a simple _random forest_ for the second layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_types = ['ENFJ','ENFP','ENTJ','ENTP','ESFJ','ESFP','ESTJ','ESTP',\n",
    "    'INFJ','INFP','INTJ','INTP','ISFJ','ISFP','ISTJ','ISTP']\n",
    "type2int = {t:i for i,t in enumerate(mbti_types)}\n",
    "int2type = {i:t for i,t in enumerate(mbti_types)}\n",
    "\n",
    "train_y2 = train_y.apply(lambda x:type2int[x]).values\n",
    "test_y2  = test_y.apply(lambda x:type2int[x]).values\n",
    "\n",
    "train_X2 = cf_predict(train_X)\n",
    "test_X2  = cf_predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I E N S T F J P I 0.7436861704088781\n",
      "E 0.7455664933820005\n",
      "N 1.0\n",
      "S 1.0\n",
      "T 1.0\n",
      "F 1.0\n",
      "J 1.0\n",
      "P 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "deg1 = {i:None for i in deg1}\n",
    "for i in deg1:\n",
    "    clf = RandomForestClassifier()\n",
    "    train_yy = [check_match(j,i) for j in train_y]\n",
    "    clf.fit(train_X2,train_yy)\n",
    "    deg1[i] = clf\n",
    "    print(i,end=' ')\n",
    "for i,clf in deg1.items():\n",
    "    test_yy = [check_match(j,i) for j in test_y]\n",
    "    print(i, accuracy_score(test_yy, clf.predict(test_X2)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
